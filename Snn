import numpy as np
import pandas as pd
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn import svm
from sklearn import metrics
import matplotlib.pyplot as plt
import seaborn as sns
import itertools

# Input value encoding: Gaussian receptive field
gauss_neuron = 12  # Each input value is encoded by 12 gaussian neurons
center = np.ones((gauss_neuron, 1))		
width = 1/15
# let the value range equal in each feautres, thus gaussian encoding center for each feature is the same
for i in range(len(center)): # let f_max=1, f_min=0. Center for each feature=i/(n - 1)
  center[i] = (2*i-3)/20  # Center of gaussian shift from left to right

x = np.arange(0, 1, 0.0001) # input variable resolution in gaussian receptive field
num_features = 2
gauss_recpt_field = np.zeros((gauss_neuron, len(x))) # gaussian receptive field for input stimulate response
spike_time = np.zeros((gauss_neuron, num_features)) # 24 gaussian encoding neurons for 2 input features
# Define gaussian receptive field for input variables entries
for i in range(gauss_neuron):
  gauss_recpt_field[i, :] = np.exp( -(x-center[i])**2/(2*width*width) ) # different shift for each gaussian function

def gauss_response(inputs):
  for i in range(num_features):
    # for each input[i]
    for j in range(gauss_neuron):
      spike_time[j, i] = gauss_recpt_field[j, inputs[i]] # input feature enter the gaussian entry, recieve response
  spikes = []
  for i in range(spike_time.shape[1]):
    spikes.extend(spike_time[:, i])
  return np.array(spikes)	

# uploas the input signals to model 
import pandas as pd
from google.colab import files
up=files.upload()
for fn in up.keys():
  print('User up files "{name}" with length {length} bytes'.format (
   name=fn, length=len(up[fn])))
  import io
data=pd.read_csv(io.StringIO(up['file.csv'].decode('utf-8')))
#data.tail()
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt
#print(data1.head(5))
X=data.iloc[:, 1:3].values
y=data.iloc[:, 0].values
labelencoder_X_1=LabelEncoder()
y=labelencoder_X_1.fit_transform(y)

# encoding of input signals 
gauss_neurons = gauss_neuron*num_features
scaler = MinMaxScaler()
X = scaler.fit_transform(X)

# There are 10000 entries in each gaussian function
X = (X*10000).astype(int) # meet the index of gaussian receptive field, drop the fourth decimal
X[X == 10000] = 9999 # convert all input value indexing to gaussian matrix entry
input_spike = np.zeros((X.shape[0], gauss_neurons)) # spike time in [0, 1]

for i in range(X.shape[0]):
  input_spike[i, :] = gauss_response(X[i, :])
input_spike[input_spike < 0.1] = 0  # set activate value < 0.1 to zero
input_spike = np.around(100*(1 - input_spike)) # convert actiate value to 0~100 timing, round the decimals
#Adjust t = 0 firing to t = 1
input_spike[input_spike == 0] = 1
x_train, x_valid, y_train, y_valid = train_test_split(input_spike, y, test_size = 0.1, stratify = y)
#y_train, y_valid = y_train.values, y_valid.values
# Start training
def heavyside(t, t_i):
  if (t - t_i) >= 0:
    return 1
  elif (t - t_i) < 0:
    return 0
def K_res(t, t_i):
  time = np.arange(0, 101, 1)
  tou_m = 15
  tou_s = tou_m/4
  V_0 = (np.exp(-(time)/tou_m) - np.exp(-(time)/tou_s)).max()  #normalization factor: divide the K() response by maximum value
  response = ( np.exp(-(t-t_i)/tou_m) - np.exp(-(t-t_i)/tou_s) )*heavyside(t, t_i)/V_0
  return response

#Model parameters
iterations = 40
num_class = 2
V = np.zeros((num_class, 100)) # potential of each output neuron
V_rest = 0
weight = np.random.random((gauss_neurons, num_class)) # connections btw input neuron and output neuron(here use single synaptic for simplicity)
threshold = 1 # set threshold: try and error
T = 100    # time encoding window
firing_time = np.zeros((gauss_neurons)) # time-to-first-spike for all gaussian neurons
Response = np.zeros((gauss_neurons))  # K(t-t_i) response for each gaussian neurons
lr = 0.005
tou_m = 15
time = np.arange(1, T+1)
accuracy = np.zeros((iterations))

# Traing of model
def train():

   for iterate in range(iterations):
      # Check the accuracy after training each classifier
      correct = 0
      val_predict = np.zeros((y_valid.shape))
      for valid_sample in range(y_valid.shape[0]):
        t_max = T*np.ones((num_class), dtype = np.int8) # default firing time: all neuron's maximum firing time at 100ms
        max_state = np.zeros((num_class)) # max potential state of each output neuron
        # Go through a time window, observe the firing history
        for t in range(T):
          for neuron in range(gauss_neurons):
            Response[neuron] = K_res(t, x_valid[valid_sample, neuron])

          #calculate PSP for output neuron
          for out_id in range(num_class):
            V[out_id, t] = np.matmul(Response,weight[:, out_id]) + V_rest

        # Find the firing timing t_max
        for out_id in range(num_class):
          fire_time = np.where((V > threshold)[out_id, :])[0]
          if len(fire_time) > 0: # set firing time between 1 to 100
            max_state[out_id] = fire_time[0] + 1
            t_max[out_id] = fire_time[0] + 1
            t_max[out_id] = t_max[out_id].astype(int)
          else:
            max_state[out_id] = T

          if t_max[out_id] < T: # if output neuron fires
            V[out_id, t_max[out_id]:] = V[out_id, t_max[out_id]]*np.exp((time[t_max[out_id]:]-time[t_max[out_id]])/tou_m)
        # We only need t_max for prediction?
        pred_class = np.argmin(t_max)
        val_predict[valid_sample] = pred_class
      acc = accuracy_score(y_valid, val_predict)
      print('Validation accuaracy at iteration {}: {}'.format(str(iterate+1), acc))

      # Training
      for sample in range(x_train.shape[0]):
        t_max = 100*np.ones((num_class), dtype = np.int8)
        max_state = np.zeros((num_class))
        for t in range(T):
          for neuron in range(gauss_neurons):
            Response[neuron] = K_res(t, x_train[sample, neuron])
          for out_id in range(num_class):
            V[out_id, t] = np.matmul(Response,weight[:, out_id]) + V_rest

        # Find the firing timing t_max
        for out_id in range(num_class):
          for timing in range(T):
            if V[out_id, timing] >= threshold:
              t_max[out_id] = timing
              max_state[out_id] = V[out_id, timing]
              break

          if t_max[out_id] < T: # if output neuron fires
            V[out_id, t_max[out_id]:] = V[out_id, t_max[out_id]]*np.exp((time[t_max[out_id]:]-time[t_max[out_id]])/tou_m)

        pred_class = np.argmin(t_max)
        # Modify weight when error occurs: stochastic learning
        target = y_train[sample] # if target class != predict class, modify the weight
        if pred_class != target:
          for out_id in range(num_class):
            if out_id == target:
              if max_state[out_id] < threshold: # if the neuron supposed to fire did not fire
                for neuron in range(gauss_neurons):
                  if x_train[sample, neuron] < t_max[out_id]: # for input neuron that contribute to the firing
                    # enhance the weight connection to the output neuron corresponding to target class
                    weight[neuron, out_id] += lr*K_res(t_max[out_id], x_train[sample, neuron])

            elif out_id != target:
              if max_state[out_id] >= threshold: # if neuron shouldn't fired are erupted to fire
                for neuron in range(gauss_neurons):
                  if x_train[sample, neuron] < t_max[out_id]:
                    # for input neuron contribute to the spike, inhibit the firing
                    weight[neuron, out_id] -= lr*K_res(t_max[out_id], x_train[sample, neuron])

        # Even if predict class is correct, if the output neuron other than target class is evoked,
        # we should inhibit the firing of these neurons
        elif pred_class == target:
          for out_id in range(num_class):
            if out_id != target:
              if max_state[out_id] >= threshold:
                for neuron in range(gauss_neurons):
                  if x_train[sample, neuron] < t_max[out_id]:
                    weight[neuron, out_id] -= lr*K_res(t_max[out_id], x_train[sample, neuron])

      # Check training accuracy
      train_predict = np.zeros(y_train.shape)
      for train_sample in range(x_train.shape[0]):
        t_max = T*np.ones((num_class), dtype = np.int8) # default firing time: all neuron's maximum firing time at 100ms
        max_state = np.zeros((num_class)) # max potential state of each output neuron
        # Go through a time window, observe the firing history
        for t in range(T):
          for neuron in range(gauss_neurons):
            Response[neuron] = K_res(t, x_train[train_sample, neuron])

          #calculate PSP for output neuron
          for out_id in range(num_class):
            V[out_id, t] = np.matmul(Response,weight[:, out_id]) + V_rest
        # Find the firing timing t_max
        for out_id in range(num_class):
          for timing in range(T):
            if V[out_id, timing] >= threshold:
              t_max[out_id] = timing
              max_state[out_id] = V[out_id, timing]
              break

          if t_max[out_id] < T: # if output neuron fires
            V[out_id, t_max[out_id]:] = V[out_id, t_max[out_id]]*np.exp((time[t_max[out_id]:]-time[t_max[out_id]])/tou_m)
        # We only need t_max for prediction?
        pred_class = np.argmin(t_max)
        train_predict[train_sample] = pred_class
      acc = accuracy_score(y_train, train_predict)
      print('Training accuaracy at iteration {}: {}'.format(str(iterate+1), acc))
train()
# to make the confusion matrix
train_predict = np.zeros(y_train.shape)
val_predict = np.zeros(y_valid.shape)
from sklearn.metrics import confusion_matrix
train_conf_matrix = confusion_matrix(y_train, train_predict)
print(train_conf_matrix)
val_conf_matrix = confusion_matrix(y_valid, val_predict)
print(val_conf_matrix)

# to upload the withheld data
# test data has the same format as your training/validation data
# Preprocess the test data similarly to the training/validation data
up = files.upload() # for validation datasets
if len(up) > 0:
    for fn in up.keys():
        print('User uploaded file "{name}" with length {length} bytes'.format(name=fn, length=len(up[fn])))
        import io
        test_data = pd.read_csv(io.StringIO(up[fn].decode('utf-8')))

def gauss_response(inputs):
    for i in range(num_features):
        input_index = int(round(inputs[i]))

        if 0 <= input_index < gauss_recpt_field.shape[1]:
            for j in range(gauss_neuron):
                spike_time[j, i] = gauss_recpt_field[j, input_index]
        else:
            pass
    spikes = []
    for i in range(spike_time.shape[1]):
        spikes.extend(spike_time[:, i])
    return np.array(spikes)

X_test = test_data.iloc[:, 1:3].values
y_test = test_data.iloc[:, 0].values
labelencoder_X_1 = LabelEncoder()
y_test = labelencoder_X_1.fit_transform(y_test)
X_test_normalized = scaler.transform(X_test)
# Convert test data to spike representations using your 'gauss_response' function
input_spike_test = np.zeros((X_test_normalized.shape[0], gauss_neurons))
for i in range(X_test_normalized.shape[0]):
    input_spike_test[i, :] = gauss_response(X_test_normalized[i, :])

input_spike_test[input_spike_test < 0.1] = 0
input_spike_test = np.around(100 * (1 - input_spike_test))
input_spike_test[input_spike_test == 0] = 1
# Make predictions on the test data
test_predictions = np.zeros(y_test.shape)
for sample in range(input_spike_test.shape[0]):
    t_max = T * np.ones((num_class), dtype=np.int8)
    max_state = np.zeros((num_class))
    for t in range(T):
        for neuron in range(gauss_neurons):
            Response[neuron] = K_res(t, input_spike_test[sample, neuron])

        for out_id in range(num_class):
            V[out_id, t] = np.matmul(Response, weight[:, out_id]) + V_rest

    for out_id in range(num_class):
        for timing in range(T):
            if V[out_id, timing] >= threshold:
                t_max[out_id] = timing
                max_state[out_id] = V[out_id, timing]
                break

        if t_max[out_id] < T:
            V[out_id, t_max[out_id]:] = V[out_id, t_max[out_id]] * np.exp(
                (time[t_max[out_id]:] - time[t_max[out_id]]) / tou_m)

    pred_class = np.argmin(t_max)
    test_predictions[sample] = pred_class

# Calculate accuracy on the test set
test_accuracy = accuracy_score(y_test, test_predictions)
print("Test Accuracy:", test_accuracy)
# confusion matrix of withheld data
conf_matrix = confusion_matrix(y_test, test_predictions)
print(conf_matrix)
import seaborn as sns
# Plot confusion matrix as heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted labels')
plt.ylabel('True labels')
plt.title('Confusion Matrix')
plt.show()

# k-fold validation 
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import accuracy_score
X_test = test_data.iloc[:, 1:3].values
y_test = test_data.iloc[:, 0].values
# Encode categorical labels if necessary
labelencoder_X_1 = LabelEncoder()
y_test = labelencoder_X_1.fit_transform(y_test)
X_test_normalized = scaler.transform(X_test)

num_folds = 10
kf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)
accuracy_scores = []
#Iterate over each fold
for fold, (train_index, test_index) in enumerate(kf.split(X, y), 1):
    print(f"Fold {fold}:")
    # Split data into train and test sets for this fold
    x_test, y_test = input_spike[test_index], y[test_index]
    # Make predictions on the test data
    test_predictions = np.zeros(y_test.shape)
    for sample in range(x_test.shape[0]):
        t_max = T * np.ones((num_class), dtype=np.int8)
        max_state = np.zeros((num_class))
        for t in range(T):
            for neuron in range(gauss_neurons):
                Response[neuron] = K_res(t, x_test[sample, neuron])

            for out_id in range(num_class):
                V[out_id, t] = np.matmul(Response, weight[:, out_id]) + V_rest

        for out_id in range(num_class):
            for timing in range(T):
                if V[out_id, timing] >= threshold:
                    t_max[out_id] = timing
                    max_state[out_id] = V[out_id, timing]
                    break

            if t_max[out_id] < T:
                V[out_id, t_max[out_id]:] = V[out_id, t_max[out_id]] * np.exp(
                    (time[t_max[out_id]:] - time[t_max[out_id]]) / tou_m)

        pred_class = np.argmin(t_max)
        test_predictions[sample] = pred_class

    # Calculate accuracy for this fold and append to accuracy_scores list
    accuracy = accuracy_score(y_test, test_predictions)
    accuracy_scores.append(accuracy)
    print("Accuracy for Fold", fold, ":", accuracy)
# calculate the average accuracy
avg_accuracy = sum(accuracy_scores) / len(accuracy_scores)
print("Average Accuracy:", avg_accuracy)
